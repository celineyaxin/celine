# 获得数据
## web_scraping.py
* 通过网页爬取投诉数据，会出现显示1000条的数据限制，因此需要转换爬取方法
## merchant_scraping.py
* 入驻的商家列表信息仍然有显示数量的反爬限制，是由于限制了查询条件
* 需要通过更改查询条件获得所有内容

# 数据预处理
* 入驻商家：在这里通过投诉文本内容的提取筛选出其中的金融类商家（支付业务提供pos机买卖服务的除外）
* 未入驻但收到投诉的商家：统一投诉对象名称为单位筛选（统计投诉量）

# 建数据库
## append.py：合并数据
* 合并爬取之后获得了2500个excel文件
* 由于数据量很大，无法合并成一个dataframe进行数据处理，会超过cpu的运行内存
* 合并为64个csv文件方便后面的数据处理工作（需要增加多个进程加速合并）

## time_split.py：按月份对于总数据进行切分每个月的数据建立一个独立的文件夹
* 根据时间类型对数据按月份进行切分

## merge_7.0.py：名称匹配
* 获得的原始数据中投诉对象和商家列表的商家名称并不完全对应，需要通过一些规律特征对数据进行匹配
* 最终需要获得的数据格式为：投诉商家、投诉编号、匹配状态

### 筛选逻辑
* 商家名称和投诉对象完全不一致：我们通过手工链接搜索一一对应商家名称和投诉对象写入文件
* 美团、哈啰和唯品会等有非常多的业务，但是投诉对象名称是统一的：根据投诉文本的内容进行筛选提取
* 商家名称和投诉对象完全匹配
* 投诉对象是商家列表中的子集
* 通过size.py查看合并情况
* 其他没匹配的通过write_csv.py处理：获得未匹配列表在原数据库中通过手工链接搜索

### size.py：在合并过程中需要查看最后的一些合并情况和合并的特征
* 缺失值
* 变量数量
* 某商家收到的投诉数量：模糊匹配之后需要与网站统计数量对应，数量级应当接近
* 匹配状态各数值的数量：每一次更新代码完全匹配的数量应该越来越多
* 筛选出各匹配状态的编号找规律完善代码：合并状态为0需要输出通过write_csv.py输出之后手工处理；对于匹配状态为1的模糊匹配也需要输出查看规律完善模糊匹配的代码

## filter_finance.py：筛选完成数据库中的金融投诉
* 完成了投诉对象和商家的匹配之后，提取出金融类企业的投诉内容

# 文本分类
## extract.py
* 在使用BERT进行文本分类之前，我们需要获得一个标记的文本
* 在获得的金融类投诉文本的数据库中，根据文本内容提取出包含“利率”，“利息”或者“贷款”的投诉文本的内容
* 因为这一段文本更容易和贷款行为相关可以提高标注的效率，这里主要是为了获得标记为“1”的文本内容
* 对于为“0”的文本在内容中会更加丰富

## classifier.py
* 使用常用的机器学习分类方法进行探索

## bert_consumer.py
* 根据数据切分训练模型

## bert_predict.py
* 对全文本预测分类

# 特征提取
## extract_information.py
* 在实证过程中需要提取文本中一些特征信息，比如金额，回复处理状态等
* 根据月份的数据库提取信息





